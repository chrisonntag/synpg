{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n96GghgHCQNC"
      },
      "source": [
        "# Preparing a custom trained PyTorch Model for Serverless Inference in AWS SageMaker\n",
        "\n",
        "This notebook serves as an documentary entry-point for deploying a custom-trained PyTorch model on AWS SageMaker on a Serverless Inference Endpoint.\n",
        "\n",
        "\"Amazon SageMaker Serverless Inference is a purpose-built inference option that enables you to deploy and scale ML models without configuring or managing any of the underlying infrastructure.\" ([Source](https://docs.aws.amazon.com/sagemaker/latest/dg/serverless-endpoints.html)).\n",
        "\n",
        "In order to use this, we need to prepare two assets:\n",
        "\n",
        "1. A Docker container, which includes all necessary dependencies like libraries and other 3rd party Python packages, and\n",
        "2. a compressed tar file, which encapsules the model artifacts (weights, vocabulary, â€¦) and an inference script, which has to offer a set of certain functions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Wi8tze7Yuc1"
      },
      "source": [
        "## Implementing the inference script\n",
        "\n",
        "SageMaker expects the inference script to offer the following functions. Check out a dummy implementation below:\n",
        "\n",
        "```\n",
        "def model_fn(model_dir):\n",
        "    \"\"\"\n",
        "    This function is the first to get executed upon a prediction request,\n",
        "    it loads the model from the disk and returns the model object which will be used later for inference.\n",
        "    \"\"\"\n",
        "    dictionary = load_dictionary(os.path.join(model_dir, 'dictionary.pkl'))\n",
        "    synpg_model = SynPG(len(dictionary), 300, word_dropout=0.4)\n",
        "    pg_model = SynPG(len(dictionary), 300, word_dropout=0.4)\n",
        "\n",
        "    return synpg_model, pg_model, bpe, dictionary\n",
        "\n",
        "\n",
        "def input_fn(request_body, request_content_type):\n",
        "    \"\"\"\n",
        "    The request_body is passed in by SageMaker and the content type is passed in\n",
        "    via an HTTP header by the client (or caller). This function then processes the\n",
        "    input data, and extracts three fields from the json body called \"sent\", \"synt\"\n",
        "    and \"tmpl\" and returns all three.\n",
        "\n",
        "    Example JSON input:\n",
        "    {\n",
        "        \"sent\": \"The quick brown fox jumps over the lazy dog\",\n",
        "        \"synt\": \"(ROOT (S (NP (DT The) (JJ quick) (JJ brown) (NN fox)) (VP (VBZ jumps) (PP (IN over) (NP (DT the) (JJ lazy) (NN dog)))))\",\n",
        "        \"tmpl\": \"(ROOT (S (S ) (, ) (CC ) (S ) (. )))\"\n",
        "    }\n",
        "    \"\"\"\n",
        "    # Extract the sent, synt and tmpl from the request\n",
        "    sent = json.loads(request_body)[\"sent\"]\n",
        "    synt = json.loads(request_body)[\"synt\"]\n",
        "    tmpl = json.loads(request_body)[\"tmpl\"]\n",
        "\n",
        "\n",
        "    return sent, synt, tmpl\n",
        "\n",
        "\n",
        "def predict_fn(input_data, model):\n",
        "    \"\"\"\n",
        "    This function takes in the input data and the model returned by the model_fn\n",
        "    It gets executed after the model_fn and its output is returned as the API response.\n",
        "    \"\"\"\n",
        "\n",
        "    synpg_model, pg_model, bpe, dictionary = model\n",
        "\n",
        "    sent, synt, tmpl = input_data\n",
        "    tmpls = template2tensor([tmpl], args['max_tmpl_len'], dictionary)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Predict using the model.    \n",
        "        \n",
        "    return output_idxs, dictionary\n",
        "\n",
        "\n",
        "def output_fn(prediction, accept):\n",
        "    \"\"\"\n",
        "    Post-processing function for model predictions. It gets executed after the predict_fn\n",
        "    and returns the prediction as json.\n",
        "    \"\"\"\n",
        "    output_idxs, dictionary = prediction\n",
        "\n",
        "    return json.dumps(output_idxs), accept\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SuweLt14ZM5H"
      },
      "source": [
        "## Creating the model tar file\n",
        "\n",
        "From PyTorch>=1.2.0 on, SageMaker requires the tar file to have a certain structure:\n",
        "```\n",
        "./\n",
        "  code/\n",
        "    inference.py\n",
        "    requirements.txt\n",
        "  model.pth\n",
        "  vocab.txt\n",
        "  ...\n",
        "```\n",
        "\n",
        "We accomplish this with\n",
        "```\n",
        "tar -czvf artifacts/model.tar.gz code/ -C model/ .\n",
        "```\n",
        "\n",
        "This can now be uploaded to your AWS S3 Bucket."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tjx9uit3YI3N"
      },
      "source": [
        "## Preparing the Dockerfile\n",
        "\n",
        "The Dockerfile in this case uses a pre-built container as a basis, which already includes the specific PyTorch version we need for this project.\n",
        "\n",
        "```\n",
        "FROM 763104351884.dkr.ecr.eu-central-1.amazonaws.com/pytorch-inference:1.2.0-gpu-py36-cu100-ubuntu16.04\n",
        "\n",
        "# Set environment variables\n",
        "# ensures that Python outputs everything that's printed directly to the terminal (so logs can be seen in real-time)\n",
        "ENV PYTHONUNBUFFERED=TRUE\n",
        "# ensures Python doesn't try to write .pyc files to disk (useful for improving performance in some scenarios)\n",
        "ENV PYTHONDONTWRITEBYTECODE=TRUE\n",
        "# Update PATH environment variable to include /opt/program directory\n",
        "ENV PATH=\"/opt/ml/code:${PATH}\"\n",
        "\n",
        "WORKDIR /opt/ml/code\n",
        "\n",
        "COPY ./requirements.txt /opt/ml/code/requirements.txt\n",
        "\n",
        "COPY ./app.py /opt/ml/code/app.py\n",
        "COPY ./code/ /opt/ml/code/\n",
        "COPY ./synpg /opt/ml/code/synpg/\n",
        "\n",
        "RUN ls -laR /opt/ml/code/*\n",
        "\n",
        "ENV SM_MODEL_DIR /opt/ml/model\n",
        "ENV SAGEMAKER_SUBMIT_DIRECTORY /opt/ml/code\n",
        "ENV SAGEMAKER_PROGRAM inference.py\n",
        "\n",
        "RUN pip install --no-cache-dir \"git+https://github.com/chrisonntag/synpg.git\"\n",
        "RUN pip install --no-cache-dir -r requirements.txt\n",
        "RUN pip freeze\n",
        "\n",
        "#EXPOSE 8080\n",
        "#ENTRYPOINT [\"gunicorn\", \"-b\", \"0.0.0.0:8080\", \"app:app\", \"--timeout\", \"180\"]\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLhXX3nkbnf6"
      },
      "source": [
        "## Pushing the Docker container to the Registry\n",
        "\n",
        "Follow the steps to create the container and push it to a new registry in AWS, which we can use later on for model deployment.\n",
        "\n",
        "Add your AWS region and account to the .ENV file in the root directory and export the variables with the following command.\n",
        "```\n",
        "export $(cat .env | xargs)\n",
        "```\n",
        "\n",
        "```\n",
        "# Authenticate Docker to an Amazon ECR registry\n",
        "aws ecr get-login-password --region $REGION | docker login --username AWS --password-stdin $DOCKER_REG.dkr.ecr.$REGION.amazonaws.com\n",
        "\n",
        "# Loging to your private Amazon ECR registry\n",
        "aws ecr get-login-password --region $REGION | docker login --username AWS --password-stdin $ACCOUNT.dkr.ecr.$REGION.amazonaws.com\n",
        "```\n",
        "\n",
        "Don't forget to create an access key for the AWC CLI.\n",
        "\n",
        "Now build the Docker image and push it to the Amazon ECR registry.\n",
        "```\n",
        "docker build -t synpg .\n",
        "```\n",
        "\n",
        "```\n",
        "# Create the AWS ECR repository\n",
        "aws ecr create-repository --repository-name synpg\n",
        "\n",
        "# Tag the image\n",
        "docker tag synpg:latest $ACCOUNT.dkr.ecr.$REGION.amazonaws.com/synpg:latest\n",
        "\n",
        "# Push the tagged image to the AWS ECR repository\n",
        "docker push $ACCOUNT.dkr.ecr.$REGION.amazonaws.com/synpg:latest\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yY0yAUrmY3_p"
      },
      "source": [
        "## Deploying the model to AWS\n",
        "\n",
        "Now that we have uploaded the model artifacts and the inference code to an S3 bucket and the Docker image pushed to our registry, we can either create an endpoint by ourselves and use ```boto3``` to create a client and invoke the endpoint:\n",
        "\n",
        "```\n",
        "runtime = boto3.client(\"sagemaker-runtime\", region_name=\"eu-central-1\")\n",
        "\n",
        "response = runtime.invoke_endpoint(\n",
        "    EndpointName=\"scpn-endpoint\",\n",
        "    ContentType=\"application/json\",\n",
        "    Body=payload\n",
        ")\n",
        "```\n",
        "\n",
        "or we deploy the model and the endpoint right out of this notebook using the SageMaker Python package."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z4gGWzWMk-AV"
      },
      "outputs": [],
      "source": [
        "!pip install boto3 awscli sagemaker datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IPLa1k06omx9"
      },
      "outputs": [],
      "source": [
        "!aws configure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yIsjgPqblCLw",
        "outputId": "a6c698b6-cfb2-481c-a3fb-57c4da962547"
      },
      "outputs": [],
      "source": [
        "import boto3\n",
        "import json\n",
        "import numpy as np\n",
        "from sagemaker.pytorch import PyTorchModel\n",
        "from sagemaker.base_serializers import JSONSerializer\n",
        "from sagemaker.base_deserializers import JSONDeserializer\n",
        "\n",
        "S3_URI = \"s3://<path to model.tar.gz>\"\n",
        "IMAGE_URI = \"<path to>synpg:latest\"\n",
        "\n",
        "# As our defined content_type will be application/json, we need to sent it as\n",
        "# that as well. So no json.dumps() here, because this would sent it as a string,\n",
        "# raising a ValueError in the input_fn of our inference script.\n",
        "payload = {\n",
        "    \"sent\": \"we will have a picnic if it is a sunny day tomorrow.\",\n",
        "    \"synt\": \"(ROOT (S (NP (PRP we)) (VP (MD will) (VP (VB have) (NP (DT a) (NN picnic)) (SBAR (IN if) (S (NP (PRP it)) (VP (VBZ is) (NP (DT a) (JJ sunny) (NN day)) (NP (NN tomorrow))))))) (. .)))\",\n",
        "    \"tmpl\": \"(ROOT (S (S ) (, ) (CC ) (S ) (. )))\"\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ZzWi2El6RZjP"
      },
      "outputs": [],
      "source": [
        "synpg_model = PyTorchModel(\n",
        "    model_data=S3_URI,\n",
        "    image_uri=IMAGE_URI,\n",
        "    role=\"SCPNS3SageMakerRole\",\n",
        "    entry_point='inference.py'\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ANX1UQZ9VGbg"
      },
      "outputs": [],
      "source": [
        "synpg_predictor = synpg_model.deploy(\n",
        "    instance_type='ml.g4dn.xlarge',\n",
        "    initial_instance_count=1,\n",
        "    serializer=JSONSerializer(),\n",
        "    deserializer=JSONDeserializer(),\n",
        "    accept='application/json',\n",
        "    content_type='application/json'\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2_SRjrEfMfj"
      },
      "source": [
        "### Invoking the endpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "eWL39Ji9TxZm",
        "outputId": "16f3ef8f-2389-472c-ffe7-11c9faea35e1"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'it is a sunny day on a day , but we will have a tomorrowland .'"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response = synpg_predictor.predict(payload, initial_args={'ContentType': 'application/json'})\n",
        "response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6jCYXjNTfREd"
      },
      "source": [
        "### Paraphrasing SST2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "3rAP9u5Yiaqi"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "sst2 = load_dataset(\"christophsonntag/sst2-constituency\", split=\"train\", streaming=True).take(6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G_t7KCOUijpm",
        "outputId": "51e8a804-46f4-440b-dc94-5bd4a601ffd8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pairs of sentences and its paraphrased version using the following template: (ROOT (S (S ) (, ) (CC ) (S ) (. )))\n",
            "\n",
            "hide new secretions from the parental units \n",
            "hide the parental secretions , and you learn from new units .\n",
            "\n",
            "\n",
            "contains no wit , only labored gags \n",
            "no wit contains been labored , but gags labored only .\n",
            "\n",
            "\n",
            "that loves its characters and communicates something rather beautiful about human nature \n",
            "the nature of that book is beautiful , and its something loves rather human characters .\n",
            "\n",
            "\n",
            "remains utterly satisfied to remain the same throughout \n",
            "the situation remains dissatisfied , but it is same to remain utterly throughout history .\n",
            "\n",
            "\n",
            "on the worst revenge-of-the-nerds clichÃ©s the filmmakers could dredge up \n",
            "the filmmakers could dredge up the filmmakers , but revengeing the folks on the worst lurch?s of revengee-toggle .\n",
            "\n",
            "\n",
            "that 's far too tragic to merit such superficial treatment \n",
            "merit is superficial , but that tragic treatment seems to be far such too .\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "syntax_template = \"(ROOT (S (S ) (, ) (CC ) (S ) (. )))\"\n",
        "\n",
        "print(f\"Pairs of sentences and its paraphrased version using the following template: {syntax_template}\\n\")\n",
        "for elem in sst2:\n",
        "  payload = {\n",
        "      \"sent\": elem[\"sentence\"],\n",
        "      \"synt\": elem[\"constituency_tree\"],\n",
        "      \"tmpl\": syntax_template\n",
        "  }\n",
        "\n",
        "  paraphrased_sentence = synpg_predictor.predict(payload, initial_args={'ContentType': 'application/json'})\n",
        "  print(f\"{elem['sentence']}\\n{paraphrased_sentence}\\n\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yO-ZbdDUfaN-"
      },
      "source": [
        "## Cleanup\n",
        "\n",
        "Make sure to delete the endpoint, in order to cut costs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "WfsMiHMyl7gu"
      },
      "outputs": [],
      "source": [
        "synpg_predictor.delete_endpoint()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
